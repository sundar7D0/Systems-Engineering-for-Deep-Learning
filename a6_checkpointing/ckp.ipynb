{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ckp.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMCh9f+nK48QQs6aNc7VqwU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"XEhJ07rJnmjI"},"source":["import torch\n","from torch.autograd import Variable\n","!pip install wandb==0.10.7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lawiQyl55Ium"},"source":["torch.cuda.memory_allocated('cuda')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KZec8-3lnqB7"},"source":["from torch.utils.checkpoint import checkpoint_sequential\n","import torch.nn as nn\n","\n","# create a simple Sequential model\n","model = nn.Sequential(\n","    nn.Linear(100, 50),\n","    nn.ReLU(),\n","    nn.Linear(50, 20),\n","    nn.ReLU(),\n","    nn.Linear(20, 5),\n","    nn.ReLU()\n",")\n","\n","# create the model inputs\n","input_var = Variable(torch.randn(1, 100), requires_grad=True)\n","\n","# set the number of checkpoint segments\n","segments = 2\n","\n","# get the modules in the model. These modules should be in the order\n","# the model should be executed\n","modules = [module for k, module in model._modules.items()]\n","\n","# now call the checkpoint API and get the output\n","out = checkpoint_sequential(modules, segments, input_var)\n","\n","# run the backwards pass on the model. For backwards pass, for simplicity purpose, \n","# we won't calculate the loss and rather backprop on out.sum()\n","model.zero_grad()\n","out.sum().backward()\n","\n","# now we save the output and parameter gradients that we will use for comparison purposes with\n","# the non-checkpointed run.\n","output_checkpointed = out.data.clone()\n","grad_checkpointed = {}\n","for name, param in model.named_parameters():\n","    grad_checkpointed[name] = param.grad.data.clone()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2suoK_hm5PVz"},"source":["model.to('cuda')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BrD1ry-2YCfo"},"source":["import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torch.utils.checkpoint as checkpoint\n","\n","def conv_bn(inp, oup, stride, conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, nlin_layer=nn.ReLU):\n","    return nn.Sequential(\n","        conv_layer(inp, oup, 3, stride, 1, bias=False),\n","        norm_layer(oup,np.sqrt(0.1)),\n","        nlin_layer(inplace=True)\n","    )\n","\n","def conv_1x1_bn(inp, oup, conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, nlin_layer=nn.ReLU):\n","    return nn.Sequential(\n","        conv_layer(inp, oup, 1, 1, 0, bias=False),\n","        norm_layer(oup,momentum=np.sqrt(0.1)),\n","        nlin_layer(inplace=True)\n","    )\n","\n","class Flatten(nn.Module):\n","  def forward(self, x):\n","    N, C, H, W = x.size() # read in N, C, H, W\n","    return x.view(N, -1)\n","\n","class Hswish(nn.Module):\n","    def __init__(self, inplace=True):\n","        super(Hswish, self).__init__()\n","        self.inplace = inplace\n","\n","    def forward(self, x):\n","        return x * F.relu6(x + 3., inplace=self.inplace) / 6.\n","\n","class Hsigmoid(nn.Module):\n","    def __init__(self, inplace=True):\n","        super(Hsigmoid, self).__init__()\n","        self.inplace = inplace\n","\n","    def forward(self, x):\n","        return F.relu6(x + 3., inplace=self.inplace) / 6.\n","\n","class SEModule(nn.Module):\n","    def __init__(self, channel, reduction=4):\n","        super(SEModule, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Sequential(\n","            nn.Linear(channel, channel // reduction, bias=False),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(channel // reduction, channel, bias=False),\n","            Hsigmoid()\n","            # nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        b, c, _, _ = x.size()\n","        y = self.avg_pool(x).view(b, c)\n","        y = self.fc(y).view(b, c, 1, 1)\n","        return x * y.expand_as(x)\n","\n","class Identity(nn.Module):\n","    def __init__(self, channel):\n","        super(Identity, self).__init__()\n","\n","    def forward(self, x):\n","        return x\n","\n","def make_divisible(x, divisible_by=8):\n","    import numpy as np\n","    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\n","\n","class FuSe(nn.Module):\n","    def __init__(self, inp, oup, kernel, stride, exp, se=False, nl='RE',j=0):\n","        super(FuSe, self).__init__()\n","        assert stride in [1, 2]\n","        assert kernel in [3, 5]\n","        padding = (kernel - 1) // 2\n","        conv_layer = nn.Conv2d\n","        norm_layer = nn.BatchNorm2d\n","        if nl == 'RE':\n","            nlin_layer = nn.ReLU # or ReLU6\n","        elif nl == 'HS':\n","            nlin_layer = Hswish\n","        else:\n","            raise NotImplementedError\n","        if se:\n","            SELayer = SEModule\n","        else:\n","            SELayer = Identity\n","        self.conv1 = conv_layer(inp, exp, 1, 1, 0, bias=False)\n","        self.bn1 = norm_layer(exp)\n","        self.nl1 = nlin_layer(inplace=True)\n","        self.conv2_h = conv_layer(exp, exp, kernel_size=(1, kernel),stride=stride, padding=(0, padding), groups=exp, bias=False)\n","        self.bn2_h = norm_layer(exp)\n","        self.conv2_v = conv_layer(exp, exp, kernel_size=(kernel, 1),stride=stride, padding=(padding, 0), groups=exp, bias=False)\n","        self.bn2_v = norm_layer(exp)\n","        self.se1 = SELayer(2*exp)\n","        self.nl2 = nlin_layer(inplace=True)\n","        self.conv3 = conv_layer(2*exp, oup, 1, 1, 0, bias=False)\n","        self.bn3 = norm_layer(oup)\n","        if j!=4 and j!=9:\n","            self.bn1.momentum=np.sqrt(self.bn1.momentum)\n","            self.bn2_h.momentum=np.sqrt(self.bn2_h.momentum)\n","            self.bn2_v.momentum=np.sqrt(self.bn2_v.momentum)\n","            self.bn3.momentum=np.sqrt(self.bn3.momentum)\n","\n","    def forward(self, x,i=0):\n","        if i==5:\n","            out = self.conv1(x)\n","            out = self.bn1(out)\n","            out = self.nl1(out)\n","            out1 = self.bn2_h(self.conv2_h(out))\n","            out2 = self.bn2_v(self.conv2_v(out))\n","            out = torch.cat([out1, out2], 1)\n","            out = self.se1(out)\n","            out = self.nl2(out)\n","            out = self.conv3(out)\n","            out = self.bn3(out)\n","        else:  \n","            out = self.conv1(x)\n","            out = self.bn1(out)\n","            out = self.nl1(out)\n","            out1 = self.bn2_h(self.conv2_h(out))\n","            out2 = self.bn2_v(self.conv2_v(out))\n","            out = torch.cat([out1, out2], 1)\n","            out = self.se1(out)\n","            out = self.nl2(out)\n","            out = self.conv3(out)\n","            out = self.bn3(out)\n","        return out\n","\n","class FuSeConv(nn.Module):\n","    def __init__(self, n_class=100, input_size=224, dropout=0.2):\n","        super(FuSeConv, self).__init__()\n","        input_channel = 16\n","        last_channel = 1024\n","        self.mem=[]\n","        self.fmem_len=0\n","        fuse_setting = [\n","           # k, exp, c,  se,     nl,  s,\n","           [3, 16,  16,  True,  'RE', 2],\n","           [3, 72,  24,  False, 'RE', 2],\n","           [3, 88,  24,  False, 'RE', 1],\n","           [5, 96,  40,  True,  'HS', 2],\n","           [5, 240, 40,  True,  'HS', 1],\n","           [5, 240, 40,  True,  'HS', 1],\n","           [5, 120, 48,  True,  'HS', 1],\n","           [5, 144, 48,  True,  'HS', 1],\n","           [5, 288, 96,  True,  'HS', 2],\n","           [5, 576, 96,  True,  'HS', 1],\n","           [5, 576, 96,  True,  'HS', 1],\n","        ]\n","        # building first layer\n","        assert input_size % 32 == 0\n","        self.features = [conv_bn(3, input_channel, 2, nlin_layer=Hswish)]\n","#       self.features[0].momentum=np.sqrt(self.features[0].momentum)\n","        self.classifier = []\n","        # building mobile blocks\n","        j=0\n","        for k, exp, c, se, nl, s in fuse_setting:\n","            output_channel = c\n","            exp_channel = exp\n","            self.features.append(FuSe(input_channel, output_channel, k, s, exp_channel, se, nl,j))\n","            input_channel = output_channel\n","            j+=1\n","        # building last several layers\n","        last_conv = 576\n","        self.features.append(conv_1x1_bn(input_channel, last_conv, nlin_layer=Hswish))\n","        #self.features.append(SEModule(last_conv))\n","        self.features.append(nn.AdaptiveAvgPool2d(1))\n","        self.features.append(nn.Conv2d(last_conv, last_channel, 1, 1, 0))\n","        self.features.append(Hswish(inplace=True))\n","        # make it nn.Sequential\n","        self.features = nn.Sequential(*self.features)\n","        # building classifier\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(p=dropout),\n","            nn.Linear(last_channel, n_class),\n","        )\n","        self._initialize_weights()\n","        self.modules = [module for k, module in self.features._modules.items()]\n","        self.cpn=[5,10,14]  #1.[0,12,14]  #[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14] #[0,12,14]  #check_pointing_numbers\n","\n","    def custom(self, module):\n","        def custom_forward(*inputs):\n","            inputs = module(inputs[0])\n","            self.mem.append(torch.cuda.memory_allocated('cuda')-self.start_mem)\n","            return inputs\n","        return custom_forward\n","\n","    def forward(self, x):\n","        torch.cuda.reset_max_memory_allocated('cuda')\n","        i=0\n","        self.start_mem=torch.cuda.memory_allocated('cuda')\n","        for _ in self.modules:\n","            if i in self.cpn:\n","                if i is 5:\n","                    x = _(x,i)\n","                else:\n","                    x = _(x)\n","                self.mem.append(torch.cuda.memory_allocated('cuda')-self.start_mem)\n","            else:\n","                x = checkpoint.checkpoint(self.custom(_),x)\n","            i+=1\n","        x = x.mean(3).mean(2)       \n","        self.mem.append(torch.cuda.memory_allocated('cuda')-self.start_mem)\n","        x = self.classifier(x)\n","        self.mem.append(torch.cuda.memory_allocated('cuda')-self.start_mem)\n","        self.fmem_len=len(self.mem)\n","        return x\n","\n","    def _initialize_weights(self):\n","        # weight initialization\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n","                if m.bias is not None:\n","                    nn.init.zeros_(m.bias)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.ones_(m.weight)\n","                nn.init.zeros_(m.bias)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                if m.bias is not None:\n","                    nn.init.zeros_(m.bias)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9tv2Uzhcvsm2"},"source":["fc=FuSeConv().to('cuda')\n","x = Variable(torch.randn(1,3,1000,1000), requires_grad=True).to('cuda')\n","start_time=time.time()\n","out=fc(x)\n","fc.zero_grad()\n","out.sum().backward(retain_graph=False)\n","fc.mem.append(torch.cuda.memory_allocated('cuda')-fc.start_mem)\n","compute_time=time.time()-start_time\n","print('time:',compute_time)\n","print('mem: ',fc.mem,fc.fmem_len)\n","#wandb.log({'compute_time':compute_time})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIZfEvzZOvic"},"source":["#peak_mem=[151164416,196726272,154889216,163175424,214085632]\n","#peak_comp=[0.1281,0.1269,0.1382,0.1248,0.1292]\n","peak_mem=[3404800,436736000,2928128,3347456,2435072,2155520]\n","peak_comp=[0.21,0.1857,0.184,0.1937,0.1468,0.1486]\n","labels=['exp1','exp2','exp3','exp4','exp5','exp6']\n","import matplotlib.pyplot as pyplot\n","Fig, ax = pyplot.subplots()\n","pyplot.rcParams['legend.numpoints'] = 1\n","for i, (mark, color) in enumerate(zip(\n","    ['s', 'o', 'D', 'v','o','s'], ['r', 'g', 'b', 'purple','y','g'])):\n","    ax.plot(peak_mem[i], peak_comp[i], color=color,\n","            marker=mark,\n","            markerfacecolor='None',\n","            markeredgecolor=color,\n","            linestyle = 'None',\n","            label=labels[i])\n","pyplot.xlabel('Peak GPU memory (in bytes)')\n","pyplot.ylabel('Compute time (in s)')\n","pyplot.title('Compute vs Memory tradeoff')\n","pyplot.legend()\n","pyplot.show()\n","#plt.plot(peak_mem,peak_comp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6k3ma0vO6yVY"},"source":["from matplotlib import pyplot as plt\n","plt.plot(fc.mem)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GV808dQTAY-9"},"source":["import wandb\n","wandb.init(project='fuse_conv_ckp',name='exp3')\n","model=FuSeConv().to('cuda')\n","wandb.watch(model,log='all')\n","input = Variable(torch.randn(1,3,1000,1000), requires_grad=True).to('cuda')\n","start_time=time.time()\n","out=model(input)\n","model.zero_grad()\n","out.sum().backward(retain_graph=False)\n","model.mem.append(torch.cuda.memory_allocated('cuda')-model.start_mem)\n","compute_time=time.time()-start_time\n","memory=model.mem\n","for _ in memory:\n","    wandb.log({'GPU_memory':_})\n","wandb.log({'Compute time':compute_time})\n","wandb.log({'Backprop_start_step':model.fmem_len})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nasQU3rodYW_"},"source":["!pip install pytorch-pretrained-bert"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BEOB6SfFZ6tF"},"source":["#bert\n","import torch\n","from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n","\n","# Load pre-trained model tokenizer (vocabulary)FloatTensor of size [batch_size, hidden_size] which is the output of a classifier pretrained on top of the hidden state associated to the first character of the input (CLF) to train on the Next-Sentence task (see BERT's paper).\n","\n","#An example on how to use this class is given i\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenized input\n","text = \"Who was Jim Henson ? Jim Henson was a puppeteer\"\n","tokenized_text = tokenizer.tokenize(text)\n","\n","# Mask a token that we will try to predict back with `BertForMaskedLM`\n","masked_index = 6\n","tokenized_text[masked_index] = '[MASK]'\n","assert tokenized_text == ['who', 'was', 'jim', 'henson', '?', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer']\n","\n","# Convert token to vocabulary indices\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n","segments_ids = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n","\n","# Convert inputs to PyTorch tensors\n","tokens_tensor = torch.tensor([indexed_tokens])\n","segments_tensors = torch.tensor([segments_ids])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PmSFALYNd8fl"},"source":["model = BertModel.from_pretrained('bert-base-uncased')\n","model.eval()\n","\n","# Predict hidden states features for each layer\n","encoded_layers, _ = model(tokens_tensor, segments_tensors)\n","# We have a hidden states for each of the 12 layers in model bert-base-uncased\n","assert len(encoded_layers) == 12"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p5PiB7Wsg7Jr"},"source":["import os\n","import logging\n","import shutil\n","import tempfile\n","import json\n","from urllib.parse import urlparse\n","from pathlib import Path\n","from typing import Optional, Tuple, Union, IO, Callable, Set\n","from hashlib import sha256\n","from functools import wraps\n","\n","from tqdm import tqdm\n","\n","import boto3\n","from botocore.exceptions import ClientError\n","import requests\n","\n","logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n","\n","PYTORCH_PRETRAINED_BERT_CACHE = Path(os.getenv('PYTORCH_PRETRAINED_BERT_CACHE',\n","                                               Path.home() / '.pytorch_pretrained_bert'))\n","\n","\n","def url_to_filename(url: str, etag: str = None) -> str:\n","    \"\"\"\n","    Convert `url` into a hashed filename in a repeatable way.\n","    If `etag` is specified, append its hash to the url's, delimited\n","    by a period.\n","    \"\"\"\n","    url_bytes = url.encode('utf-8')\n","    url_hash = sha256(url_bytes)\n","    filename = url_hash.hexdigest()\n","\n","    if etag:\n","        etag_bytes = etag.encode('utf-8')\n","        etag_hash = sha256(etag_bytes)\n","        filename += '.' + etag_hash.hexdigest()\n","\n","    return filename\n","\n","\n","def filename_to_url(filename: str, cache_dir: str = None) -> Tuple[str, str]:\n","    \"\"\"\n","    Return the url and etag (which may be ``None``) stored for `filename`.\n","    Raise ``FileNotFoundError`` if `filename` or its stored metadata do not exist.\n","    \"\"\"\n","    if cache_dir is None:\n","        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n","\n","    cache_path = os.path.join(cache_dir, filename)\n","    if not os.path.exists(cache_path):\n","        raise FileNotFoundError(\"file {} not found\".format(cache_path))\n","\n","    meta_path = cache_path + '.json'\n","    if not os.path.exists(meta_path):\n","        raise FileNotFoundError(\"file {} not found\".format(meta_path))\n","\n","    with open(meta_path) as meta_file:\n","        metadata = json.load(meta_file)\n","    url = metadata['url']\n","    etag = metadata['etag']\n","\n","    return url, etag\n","\n","\n","def cached_path(url_or_filename: Union[str, Path], cache_dir: str = None) -> str:\n","    \"\"\"\n","    Given something that might be a URL (or might be a local path),\n","    determine which. If it's a URL, download the file and cache it, and\n","    return the path to the cached file. If it's already a local path,\n","    make sure the file exists and then return the path.\n","    \"\"\"\n","    if cache_dir is None:\n","        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n","    if isinstance(url_or_filename, Path):\n","        url_or_filename = str(url_or_filename)\n","\n","    parsed = urlparse(url_or_filename)\n","\n","    if parsed.scheme in ('http', 'https', 's3'):\n","        # URL, so get it from the cache (downloading if necessary)\n","        return get_from_cache(url_or_filename, cache_dir)\n","    elif os.path.exists(url_or_filename):\n","        # File, and it exists.\n","        return url_or_filename\n","    elif parsed.scheme == '':\n","        # File, but it doesn't exist.\n","        raise FileNotFoundError(\"file {} not found\".format(url_or_filename))\n","    else:\n","        # Something unknown\n","        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\n","\n","\n","def split_s3_path(url: str) -> Tuple[str, str]:\n","    \"\"\"Split a full s3 path into the bucket name and path.\"\"\"\n","    parsed = urlparse(url)\n","    if not parsed.netloc or not parsed.path:\n","        raise ValueError(\"bad s3 path {}\".format(url))\n","    bucket_name = parsed.netloc\n","    s3_path = parsed.path\n","    # Remove '/' at beginning of path.\n","    if s3_path.startswith(\"/\"):\n","        s3_path = s3_path[1:]\n","    return bucket_name, s3_path\n","\n","\n","def s3_request(func: Callable):\n","    \"\"\"\n","    Wrapper function for s3 requests in order to create more helpful error\n","    messages.\n","    \"\"\"\n","\n","    @wraps(func)\n","    def wrapper(url: str, *args, **kwargs):\n","        try:\n","            return func(url, *args, **kwargs)\n","        except ClientError as exc:\n","            if int(exc.response[\"Error\"][\"Code\"]) == 404:\n","                raise FileNotFoundError(\"file {} not found\".format(url))\n","            else:\n","                raise\n","\n","    return wrapper\n","\n","\n","@s3_request\n","def s3_etag(url: str) -> Optional[str]:\n","    \"\"\"Check ETag on S3 object.\"\"\"\n","    s3_resource = boto3.resource(\"s3\")\n","    bucket_name, s3_path = split_s3_path(url)\n","    s3_object = s3_resource.Object(bucket_name, s3_path)\n","    return s3_object.e_tag\n","\n","\n","@s3_request\n","def s3_get(url: str, temp_file: IO) -> None:\n","    \"\"\"Pull a file directly from S3.\"\"\"\n","    s3_resource = boto3.resource(\"s3\")\n","    bucket_name, s3_path = split_s3_path(url)\n","    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n","\n","\n","def http_get(url: str, temp_file: IO) -> None:\n","    req = requests.get(url, stream=True)\n","    content_length = req.headers.get('Content-Length')\n","    total = int(content_length) if content_length is not None else None\n","    progress = tqdm(unit=\"B\", total=total)\n","    for chunk in req.iter_content(chunk_size=1024):\n","        if chunk: # filter out keep-alive new chunks\n","            progress.update(len(chunk))\n","            temp_file.write(chunk)\n","    progress.close()\n","\n","\n","def get_from_cache(url: str, cache_dir: str = None) -> str:\n","    \"\"\"\n","    Given a URL, look for the corresponding dataset in the local cache.\n","    If it's not there, download it. Then return the path to the cached file.\n","    \"\"\"\n","    if cache_dir is None:\n","        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n","\n","    os.makedirs(cache_dir, exist_ok=True)\n","\n","    # Get eTag to add to filename, if it exists.\n","    if url.startswith(\"s3://\"):\n","        etag = s3_etag(url)\n","    else:\n","        response = requests.head(url, allow_redirects=True)\n","        if response.status_code != 200:\n","            raise IOError(\"HEAD request failed for url {} with status code {}\"\n","                          .format(url, response.status_code))\n","        etag = response.headers.get(\"ETag\")\n","\n","    filename = url_to_filename(url, etag)\n","\n","    # get cache path to put the file\n","    cache_path = os.path.join(cache_dir, filename)\n","\n","    if not os.path.exists(cache_path):\n","        # Download to temporary file, then copy to cache dir once finished.\n","        # Otherwise you get corrupt cache entries if the download gets interrupted.\n","        with tempfile.NamedTemporaryFile() as temp_file:\n","            logger.info(\"%s not found in cache, downloading to %s\", url, temp_file.name)\n","\n","            # GET file object\n","            if url.startswith(\"s3://\"):\n","                s3_get(url, temp_file)\n","            else:\n","                http_get(url, temp_file)\n","\n","            # we are copying the file before closing it, so flush to avoid truncation\n","            temp_file.flush()\n","            # shutil.copyfileobj() starts at the current position, so go to the start\n","            temp_file.seek(0)\n","\n","            logger.info(\"copying %s to cache at %s\", temp_file.name, cache_path)\n","            with open(cache_path, 'wb') as cache_file:\n","                shutil.copyfileobj(temp_file, cache_file)\n","\n","            logger.info(\"creating metadata file for %s\", cache_path)\n","            meta = {'url': url, 'etag': etag}\n","            meta_path = cache_path + '.json'\n","            with open(meta_path, 'w') as meta_file:\n","                json.dump(meta, meta_file)\n","\n","            logger.info(\"removing temp file %s\", temp_file.name)\n","\n","    return cache_path\n","\n","\n","def read_set_from_file(filename: str) -> Set[str]:\n","    '''\n","    Extract a de-duped collection (set) of text from a file.\n","    Expected file format is one item per line.\n","    '''\n","    collection = set()\n","    with open(filename, 'r') as file_:\n","        for line in file_:\n","            collection.add(line.rstrip())\n","    return collection\n","\n","\n","def get_file_extension(path: str, dot=True, lower: bool = True):\n","    ext = os.path.splitext(path)[1]\n","    ext = ext if dot else ext[1:]\n","    return ext.lower() if lower else ext"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BDEDeI4NeB0o"},"source":["\"\"\"PyTorch BERT model.\"\"\"\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import os\n","import copy\n","import json\n","import math\n","import logging\n","import tarfile\n","import tempfile\n","import shutil\n","\n","import torch\n","from torch import nn\n","from torch.nn import CrossEntropyLoss\n","\n","#from .file_utils import cached_path\n","\n","logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n","                    datefmt = '%m/%d/%Y %H:%M:%S',\n","                    level = logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","PRETRAINED_MODEL_ARCHIVE_MAP = {\n","    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\n","    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\n","    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\n","    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz\",\n","    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz\",\n","    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz\",\n","    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\n","}\n","CONFIG_NAME = 'bert_config.json'\n","WEIGHTS_NAME = 'pytorch_model.bin'\n","\n","def gelu(x):\n","    \"\"\"Implementation of the gelu activation function.\n","        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n","        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n","    \"\"\"\n","    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n","\n","\n","def swish(x):\n","    return x * torch.sigmoid(x)\n","\n","\n","ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n","\n","\n","class BertConfig(object):\n","    \"\"\"Configuration class to store the configuration of a `BertModel`.\n","    \"\"\"\n","    def __init__(self,\n","                 vocab_size_or_config_json_file,\n","                 hidden_size=768,\n","                 num_hidden_layers=12,\n","                 num_attention_heads=12,\n","                 intermediate_size=3072,\n","                 hidden_act=\"gelu\",\n","                 hidden_dropout_prob=0.1,\n","                 attention_probs_dropout_prob=0.1,\n","                 max_position_embeddings=512,\n","                 type_vocab_size=2,\n","                 initializer_range=0.02):\n","        \"\"\"Constructs BertConfig.\n","        Args:\n","            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n","            hidden_size: Size of the encoder layers and the pooler layer.\n","            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n","            num_attention_heads: Number of attention heads for each attention layer in\n","                the Transformer encoder.\n","            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n","                layer in the Transformer encoder.\n","            hidden_act: The non-linear activation function (function or string) in the\n","                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n","            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n","                layers in the embeddings, encoder, and pooler.\n","            attention_probs_dropout_prob: The dropout ratio for the attention\n","                probabilities.\n","            max_position_embeddings: The maximum sequence length that this model might\n","                ever be used with. Typically set this to something large just in case\n","                (e.g., 512 or 1024 or 2048).\n","            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n","                `BertModel`.\n","            initializer_range: The sttdev of the truncated_normal_initializer for\n","                initializing all weight matrices.\n","        \"\"\"\n","        if isinstance(vocab_size_or_config_json_file, str):\n","            with open(vocab_size_or_config_json_file, \"r\") as reader:\n","                json_config = json.loads(reader.read())\n","            for key, value in json_config.items():\n","                self.__dict__[key] = value\n","        elif isinstance(vocab_size_or_config_json_file, int):\n","            self.vocab_size = vocab_size_or_config_json_file\n","            self.hidden_size = hidden_size\n","            self.num_hidden_layers = num_hidden_layers\n","            self.num_attention_heads = num_attention_heads\n","            self.hidden_act = hidden_act\n","            self.intermediate_size = intermediate_size\n","            self.hidden_dropout_prob = hidden_dropout_prob\n","            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n","            self.max_position_embeddings = max_position_embeddings\n","            self.type_vocab_size = type_vocab_size\n","            self.initializer_range = initializer_range\n","        else:\n","            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n","                             \"or the path to a pretrained model config file (str)\")\n","\n","    @classmethod\n","    def from_dict(cls, json_object):\n","        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n","        config = BertConfig(vocab_size_or_config_json_file=-1)\n","        for key, value in json_object.items():\n","            config.__dict__[key] = value\n","        return config\n","\n","    @classmethod\n","    def from_json_file(cls, json_file):\n","        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n","        with open(json_file, \"r\") as reader:\n","            text = reader.read()\n","        return cls.from_dict(json.loads(text))\n","\n","    def __repr__(self):\n","        return str(self.to_json_string())\n","\n","    def to_dict(self):\n","        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n","        output = copy.deepcopy(self.__dict__)\n","        return output\n","\n","    def to_json_string(self):\n","        \"\"\"Serializes this instance to a JSON string.\"\"\"\n","        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n","\n","\n","class BertLayerNorm(nn.Module):\n","    def __init__(self, config, variance_epsilon=1e-12):\n","        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n","        \"\"\"\n","        super(BertLayerNorm, self).__init__()\n","        self.gamma = nn.Parameter(torch.ones(config.hidden_size))\n","        self.beta = nn.Parameter(torch.zeros(config.hidden_size))\n","        self.variance_epsilon = variance_epsilon\n","\n","    def forward(self, x):\n","        u = x.mean(-1, keepdim=True)\n","        s = (x - u).pow(2).mean(-1, keepdim=True)\n","        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n","        return self.gamma * x + self.beta\n","\n","\n","class BertEmbeddings(nn.Module):\n","    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n","    \"\"\"\n","    def __init__(self, config):\n","        super(BertEmbeddings, self).__init__()\n","        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n","        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n","        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n","\n","        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n","        # any TensorFlow checkpoint file\n","        self.LayerNorm = BertLayerNorm(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, input_ids, token_type_ids=None):\n","        seq_length = input_ids.size(1)\n","        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n","        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        words_embeddings = self.word_embeddings(input_ids)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n","\n","        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings\n","\n","\n","class BertSelfAttention(nn.Module):\n","    def __init__(self, config):\n","        super(BertSelfAttention, self).__init__()\n","        if config.hidden_size % config.num_attention_heads != 0:\n","            raise ValueError(\n","                \"The hidden size (%d) is not a multiple of the number of attention \"\n","                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n","        self.num_attention_heads = config.num_attention_heads\n","        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","\n","        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n","        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n","        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n","\n","        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n","\n","    def transpose_for_scores(self, x):\n","        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n","        x = x.view(*new_x_shape)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, hidden_states, attention_mask):\n","        mixed_query_layer = self.query(hidden_states)\n","        mixed_key_layer = self.key(hidden_states)\n","        mixed_value_layer = self.value(hidden_states)\n","\n","        query_layer = self.transpose_for_scores(mixed_query_layer)\n","        key_layer = self.transpose_for_scores(mixed_key_layer)\n","        value_layer = self.transpose_for_scores(mixed_value_layer)\n","\n","        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n","        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n","        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n","        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n","        attention_scores = attention_scores + attention_mask\n","\n","        # Normalize the attention scores to probabilities.\n","        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n","\n","        # This is actually dropping out entire tokens to attend to, which might\n","        # seem a bit unusual, but is taken from the original Transformer paper.\n","        attention_probs = self.dropout(attention_probs)\n","\n","        context_layer = torch.matmul(attention_probs, value_layer)\n","        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n","        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n","        context_layer = context_layer.view(*new_context_layer_shape)\n","        return context_layer\n","\n","\n","class BertSelfOutput(nn.Module):\n","    def __init__(self, config):\n","        super(BertSelfOutput, self).__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.LayerNorm = BertLayerNorm(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, hidden_states, input_tensor):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","        return hidden_states\n","\n","\n","class BertAttention(nn.Module):\n","    def __init__(self, config):\n","        super(BertAttention, self).__init__()\n","        self.self = BertSelfAttention(config)\n","        self.output = BertSelfOutput(config)\n","\n","    def forward(self, input_tensor, attention_mask):\n","        self_output = self.self(input_tensor, attention_mask)\n","        attention_output = self.output(self_output, input_tensor)\n","        return attention_output\n","\n","\n","class BertIntermediate(nn.Module):\n","    def __init__(self, config):\n","        super(BertIntermediate, self).__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n","        self.intermediate_act_fn = ACT2FN[config.hidden_act] \\\n","            if isinstance(config.hidden_act, str) else config.hidden_act\n","\n","    def forward(self, hidden_states):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.intermediate_act_fn(hidden_states)\n","        return hidden_states\n","\n","\n","class BertOutput(nn.Module):\n","    def __init__(self, config):\n","        super(BertOutput, self).__init__()\n","        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n","        self.LayerNorm = BertLayerNorm(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, hidden_states, input_tensor):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","        return hidden_states\n","\n","\n","class BertLayer(nn.Module):\n","    def __init__(self, config):\n","        super(BertLayer, self).__init__()\n","        self.attention = BertAttention(config)\n","        self.intermediate = BertIntermediate(config)\n","        self.output = BertOutput(config)\n","        self.ckpn=[5,11]\n","\n","    def custom(self, module):\n","        def custom_forward(*inputs):\n","            global mem, start_mem\n","            print('In backprop!')\n","            inputs = module(inputs[0],inputs[1])\n","            print('inside custom: ',torch.cuda.memory_allocated('cuda'),torch.cuda.memory_allocated('cuda')-start_mem,inputs.device,start_mem)\n","            mem.append(torch.cuda.memory_allocated('cuda')-start_mem)\n","            return inputs\n","        return custom_forward\n","\n","    def forward(self, hidden_states, attention_mask,i):\n","        if i in self.ckpn:\n","            attention_output = self.attention(hidden_states,attention_mask)\n","        else:\n","            attention_output = checkpoint.checkpoint(self.custom(self.attention),hidden_states,attention_mask)\n","#        if i in self.ckpn:\n","#            intermediate_output=self.intermediate(attention_output)\n","#        else:\n","        intermediate_output = checkpoint.checkpoint(self.intermediate,attention_output)\n","        layer_output = checkpoint.checkpoint(self.output,intermediate_output, attention_output)\n","        mem.append(torch.cuda.memory_allocated('cuda')-start_mem)\n","        return layer_output\n","\n","\n","class BertEncoder(nn.Module):\n","    def __init__(self, config):\n","        super(BertEncoder, self).__init__()\n","        layer = BertLayer(config)\n","        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])    \n","        self.ckpn=[2,11]  #[3,6,11]  #$$\n","\n","    def custom(self, module):\n","        def custom_forward(*inputs):\n","            global mem, start_mem\n","            print('In backprop!')\n","            inputs = module(inputs[0],inputs[1])\n","            print('inside custom: ',torch.cuda.memory_allocated('cuda'),torch.cuda.memory_allocated('cuda')-start_mem,inputs.device,start_mem)\n","            mem.append(torch.cuda.memory_allocated('cuda')-start_mem)\n","            return inputs\n","        return custom_forward\n","\n","    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n","        global mem, start_mem, fmem_len\n","        all_encoder_layers = []\n","        i=0\n","        for layer_module in self.layer:\n","#            if i in self.ckpn:\n","#                print(i,' is checkpointed!')\n","            hidden_states = layer_module(hidden_states, attention_mask,i)\n","#            else:\n","#                hidden_states = torch.utils.checkpoint.checkpoint(self.custom(layer_module), hidden_states, attention_mask)\n","            if output_all_encoded_layers:\n","                all_encoder_layers.append(hidden_states)\n","            i+=1\n","        if not output_all_encoded_layers:\n","            all_encoder_layers.append(hidden_states)\n","        fmem_len=len(mem)\n","        print('Done with forward pass!')\n","        return all_encoder_layers\n","\n","class BertPooler(nn.Module):\n","    def __init__(self, config):\n","        super(BertPooler, self).__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.activation = nn.Tanh()\n","\n","    def forward(self, hidden_states):\n","        # We \"pool\" the model by simply taking the hidden state corresponding\n","        # to the first token.\n","        first_token_tensor = hidden_states[:, 0]\n","        pooled_output = self.dense(first_token_tensor)\n","        pooled_output = self.activation(pooled_output)\n","        return pooled_output\n","\n","\n","class BertPredictionHeadTransform(nn.Module):\n","    def __init__(self, config):\n","        super(BertPredictionHeadTransform, self).__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.transform_act_fn = ACT2FN[config.hidden_act] \\\n","            if isinstance(config.hidden_act, str) else config.hidden_act\n","        self.LayerNorm = BertLayerNorm(config)\n","\n","    def forward(self, hidden_states):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.transform_act_fn(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states)\n","        return hidden_states\n","\n","\n","class BertLMPredictionHead(nn.Module):\n","    def __init__(self, config, bert_model_embedding_weights):\n","        super(BertLMPredictionHead, self).__init__()\n","        self.transform = BertPredictionHeadTransform(config)\n","\n","        # The output weights are the same as the input embeddings, but there is\n","        # an output-only bias for each token.\n","        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n","                                 bert_model_embedding_weights.size(0),\n","                                 bias=False)\n","        self.decoder.weight = bert_model_embedding_weights\n","        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n","\n","    def forward(self, hidden_states):\n","        hidden_states = self.transform(hidden_states)\n","        hidden_states = self.decoder(hidden_states) + self.bias\n","        return hidden_states\n","\n","\n","class BertOnlyMLMHead(nn.Module):\n","    def __init__(self, config, bert_model_embedding_weights):\n","        super(BertOnlyMLMHead, self).__init__()\n","        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n","\n","    def forward(self, sequence_output):\n","        prediction_scores = self.predictions(sequence_output)\n","        return prediction_scores\n","\n","\n","class BertOnlyNSPHead(nn.Module):\n","    def __init__(self, config):\n","        super(BertOnlyNSPHead, self).__init__()\n","        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n","\n","    def forward(self, pooled_output):\n","        seq_relationship_score = self.seq_relationship(pooled_output)\n","        return seq_relationship_score\n","\n","\n","class BertPreTrainingHeads(nn.Module):\n","    def __init__(self, config, bert_model_embedding_weights):\n","        super(BertPreTrainingHeads, self).__init__()\n","        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n","        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n","\n","    def forward(self, sequence_output, pooled_output):\n","        prediction_scores = self.predictions(sequence_output)\n","        seq_relationship_score = self.seq_relationship(pooled_output)\n","        return prediction_scores, seq_relationship_score\n","\n","\n","class PreTrainedBertModel(nn.Module):\n","    \"\"\" An abstract class to handle weights initialization and\n","        a simple interface for dowloading and loading pretrained models.\n","    \"\"\"\n","    def __init__(self, config, *inputs, **kwargs):\n","        super(PreTrainedBertModel, self).__init__()\n","        if not isinstance(config, BertConfig):\n","            raise ValueError(\n","                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n","                \"To create a model from a Google pretrained model use \"\n","                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n","                    self.__class__.__name__, self.__class__.__name__\n","                ))\n","        self.config = config\n","\n","    def init_bert_weights(self, module):\n","        \"\"\" Initialize the weights.\n","        \"\"\"\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            # Slightly different from the TF version which uses truncated_normal for initialization\n","            # cf https://github.com/pytorch/pytorch/pull/5617\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","        elif isinstance(module, BertLayerNorm):\n","            module.beta.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            module.gamma.data.normal_(mean=0.0, std=self.config.initializer_range)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","    @classmethod\n","    def from_pretrained(cls, pretrained_model_name, cache_dir=None, *inputs, **kwargs):\n","        \"\"\"\n","        Instantiate a PreTrainedBertModel from a pre-trained model file.\n","        Download and cache the pre-trained model file if needed.\n","        \n","        Params:\n","            pretrained_model_name: either:\n","                - a str with the name of a pre-trained model to load selected in the list of:\n","                    . `bert-base-uncased`\n","                    . `bert-large-uncased`\n","                    . `bert-base-cased`\n","                    . `bert-base-multilingual`\n","                    . `bert-base-chinese`\n","                - a path or url to a pretrained model archive containing:\n","                    . `bert_config.json` a configuration file for the model\n","                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n","            *inputs, **kwargs: additional input for the specific Bert class\n","                (ex: num_labels for BertForSequenceClassification)\n","        \"\"\"\n","        if pretrained_model_name in PRETRAINED_MODEL_ARCHIVE_MAP:\n","            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name]\n","        else:\n","            archive_file = pretrained_model_name\n","        # redirect to the cache, if necessary\n","        try:\n","            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n","        except FileNotFoundError:\n","            logger.error(\n","                \"Model name '{}' was not found in model name list ({}). \"\n","                \"We assumed '{}' was a path or url but couldn't find any file \"\n","                \"associated to this path or url.\".format(\n","                    pretrained_model_name,\n","                    ', '.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n","                    archive_file))\n","            return None\n","        if resolved_archive_file == archive_file:\n","            logger.info(\"loading archive file {}\".format(archive_file))\n","        else:\n","            logger.info(\"loading archive file {} from cache at {}\".format(\n","                archive_file, resolved_archive_file))\n","        tempdir = None\n","        if os.path.isdir(resolved_archive_file):\n","            serialization_dir = resolved_archive_file\n","        else:\n","            # Extract archive to temp dir\n","            tempdir = tempfile.mkdtemp()\n","            logger.info(\"extracting archive file {} to temp dir {}\".format(\n","                resolved_archive_file, tempdir))\n","            with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n","                archive.extractall(tempdir)\n","            serialization_dir = tempdir\n","        # Load config\n","        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n","        config = BertConfig.from_json_file(config_file)\n","        logger.info(\"Model config {}\".format(config))\n","        # Instantiate model.\n","        model = cls(config, *inputs, **kwargs)\n","        weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n","        state_dict = torch.load(weights_path)\n","\n","        missing_keys = []\n","        unexpected_keys = []\n","        error_msgs = []\n","        # copy state_dict so _load_from_state_dict can modify it\n","        metadata = getattr(state_dict, '_metadata', None)\n","        state_dict = state_dict.copy()\n","        if metadata is not None:\n","            state_dict._metadata = metadata\n","\n","        def load(module, prefix=''):\n","            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n","            module._load_from_state_dict(\n","                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n","            for name, child in module._modules.items():\n","                if child is not None:\n","                    load(child, prefix + name + '.')\n","        load(model, prefix='' if hasattr(model, 'bert') else 'bert.')\n","        if len(missing_keys) > 0:\n","            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n","                model.__class__.__name__, missing_keys))\n","        if len(unexpected_keys) > 0:\n","            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n","                model.__class__.__name__, unexpected_keys))\n","        if tempdir:\n","            # Clean up temp dir\n","            shutil.rmtree(tempdir)\n","        return model\n","\n","\n","class BertModel(PreTrainedBertModel):\n","    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n","    Params:\n","        config: a BertConfig class instance with the configuration to build a new model\n","    Inputs:\n","        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n","            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n","            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n","        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n","            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n","            a `sentence B` token (see BERT paper for more details).\n","        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n","            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n","            input sequence length in the current batch. It's the mask that we typically use for attention when\n","            a batch has varying length sentences.\n","        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n","    Outputs: Tuple of (encoded_layers, pooled_output)\n","        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n","            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n","                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n","                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n","            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n","                to the last attention block of shape [batch_size, sequence_length, hidden_size],\n","        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n","            classifier pretrained on top of the hidden state associated to the first character of the\n","            input (`CLF`) to train on the Next-Sentence task (see BERT's paper).\n","    Example usage:\n","    ```python\n","    # Already been converted into WordPiece token ids\n","    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n","    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n","    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n","    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n","        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n","    model = modeling.BertModel(config=config)\n","    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n","    ```\n","    \"\"\"\n","    def __init__(self, config):\n","        super(BertModel, self).__init__(config)\n","        self.embeddings = BertEmbeddings(config)\n","        self.encoder = BertEncoder(config)\n","        self.pooler = BertPooler(config)\n","        self.apply(self.init_bert_weights)\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True):\n","        if attention_mask is None:\n","            attention_mask = torch.ones_like(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        # We create a 3D attention mask from a 2D tensor mask.\n","        # Sizes are [batch_size, 1, 1, to_seq_length]\n","        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n","        # this attention mask is more simple than the triangular masking of causal attention\n","        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n","\n","        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","        # masked positions, this operation will create a tensor which is 0.0 for\n","        # positions we want to attend and -10000.0 for masked positions.\n","        # Since we are adding it to the raw scores before the softmax, this is\n","        # effectively the same as removing these entirely.\n","        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        embedding_output = checkpoint.checkpoint(self.embeddings,input_ids, token_type_ids)\n","        encoded_layers = self.encoder(embedding_output,\n","                                      extended_attention_mask,\n","                                      output_all_encoded_layers=output_all_encoded_layers)\n","        sequence_output = encoded_layers[-1]\n","        pooled_output = self.pooler,sequence_output\n","        if not output_all_encoded_layers:\n","            encoded_layers = encoded_layers[-1]\n","        return encoded_layers, pooled_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d2y05RqYhDuV"},"source":["mem,fmem_len=[],0\n","#mem.append(torch.cuda.memory_allocated('cuda')-start_mem)\n","model=BertModel.from_pretrained('bert-base-uncased').to('cuda')\n","torch.cuda.reset_max_memory_allocated('cuda')\n","start_mem=torch.cuda.memory_allocated('cuda')\n","mem.append(torch.cuda.memory_allocated('cuda')-start_mem)\n","start_time=time.time()\n","encoded_layers, _ = model(tokens_tensor.to('cuda'), segments_tensors.to('cuda'))\n","ftime=time.time()-start_time\n","mem.append(torch.cuda.memory_allocated('cuda')-start_mem)\n","fmem_len=len(mem)\n","loss=sum(sum(sum(sum(encoded_layers))))\n","#mem.append(torch.cuda.memory_allocated('cuda')-start_mem)\n","model.zero_grad()\n","mem.append(torch.cuda.memory_allocated('cuda')-start_mem)\n","loss.backward()\n","btime=time.time()-(start_time+ftime)\n","mem.append(torch.cuda.memory_allocated('cuda')-start_mem)\n","#del loss\n","mem.append(torch.cuda.memory_allocated('cuda')-start_mem)\n","#mem.append(torch.cuda.memory_allocated('cuda')-start_mem)\n","# We have a hidden states for each of the 12 layers in model bert-base-uncased\n","assert len(encoded_layers) == 12\n","ttime=time.time()-start_time\n","print('f,b,t:',ftime,btime,ttime)\n","print('mem: ',mem)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ihf87WJM-WFm"},"source":["print(mem)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yo777mNf8qpH"},"source":["import wandb\n","wandb.init(project='bert_model_ckp',name='exp6')\n","mem,fmem_len=[],0\n","model=BertModel.from_pretrained('bert-base-uncased').to('cuda')\n","wandb.watch(model,log='all')\n","torch.cuda.reset_max_memory_allocated('cuda')\n","start_mem=torch.cuda.memory_allocated('cuda')\n","mem.append(torch.cuda.memory_allocated('cuda')-start_mem)\n","start_time=time.time()\n","encoded_layers, _ = model(tokens_tensor.to('cuda'), segments_tensors.to('cuda'))\n","ftime=time.time()-start_time\n","mem.append(torch.cuda.memory_allocated('cuda')-start_mem)\n","loss=sum(sum(sum(sum(encoded_layers))))\n","model.zero_grad()\n","loss.backward(retain_graph=False)\n","btime=time.time()-(start_time+ftime)\n","mem.append(torch.cuda.memory_allocated('cuda')-start_mem)\n","assert len(encoded_layers) == 12\n","ttime=time.time()-start_time\n","print('Forward pass time:,Back-prop time,Total time:',ftime,btime,ttime)\n","print('GPU memory over time (in bytes): ',mem)\n","print('Forward pass length: ',fmem_len)\n","for _ in mem[0:fmem_len]:\n","    wandb.log({'Forward pass GPU_memory':_})\n","for _ in mem[fmem_len:]:\n","    wandb.log({'Backprop GPU_memory':_})\n","wandb.log({'Forward pass time':ftime,'Back-prop time':btime,'Total time':ttime})\n","#wandb.log({'Backprop_start_step':fmem_len})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uEPQqilrjlaS"},"source":["print(mem)\n","print(torch.cuda.memory_allocated('cuda')-start_mem)\n","#loss.backward()\n","#loss.item()\n","#torch.cuda.empty_cache() \n","#print(torch.cuda.memory_allocated('cuda')-start_mem)\n","#del encoded_layers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PzkDQGVM0l6V"},"source":["#del model\n","torch.cuda.empty_cache()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86WlWJJiyNfX"},"source":["from matplotlib import pyplot as plt\n","#plt.plot(mem[0:fmem_len])\n","plt.plot(mem[fmem_len:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bptWiNVJxEXu"},"source":["mem[24]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eEqJgK2zJpDT"},"source":["memory=[3404800,436736000,2928128,3347456,2435072]\n","compute=[0.21,0.1857,0.184,0.1937,0.1468]\n","#labels=['exp1']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sLsmUht43dbI"},"source":["import torch\n","import torchvision.models as models\n","import torch.autograd.profiler as profiler\n","y=fc(x)\n","with profiler.profile(profile_memory=True, record_shapes=True) as prof:\n","    y=fc(x)\n","#    y.sum().backward()\n","print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dBMQCB9CyUQH"},"source":["print(out.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HEC7_16VYiX1"},"source":["fuse_conv=FuSeConv()\n","fuse_conv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oG-kTUMPZ8ll"},"source":["\n","import torch.nn as nn\n","torch.cuda.reset_max_memory_allocated('cuda')\n","# create a simple Sequential model\n","model = nn.Sequential(\n","    nn.Linear(100, 50),\n","    nn.ReLU(),\n","    nn.Linear(50, 20),\n","    nn.ReLU(),\n","    nn.Linear(20, 5),\n","    nn.ReLU()\n",")\n","model.to('cuda')\n","# create the model inputs\n","input_var = Variable(torch.randn(1, 100), requires_grad=True).to('cuda')\n","# set the number of checkpoint segments\n","segments = 2\n","# get the modules in the model. These modules should be in the order\n","# the model should be executed\n","modules = [module for k, module in model._modules.items()]\n","print(torch.cuda.memory_allocated('cuda'))\n","out = checkpoint_sequential(modules, segments, input_var)\n","print(torch.cuda.memory_allocated('cuda'))\n","\n","# run the backwards pass on the model. For backwards pass, for simplicity purpose, \n","# we won't calculate the loss and rather backprop on out.sum()\n","model.zero_grad()\n","out.sum().backward()\n","\n","# now we save the output and parameter gradients that we will use for comparison purposes with\n","# the non-checkpointed run.\n","output_checkpointed = out.data.clone()\n","grad_checkpointed = {}\n","for name, param in model.named_parameters():\n","    grad_checkpointed[name] = param.grad.data.clone()\n","#rad_checkpointed\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8E1A0CnetF_"},"source":["torch.cuda.empty_cache() \n","torch.cuda.memory_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-cO0MAP9bHc1"},"source":["torch.cuda.reset_max_memory_allocated('cuda')\n","original = model\n","original.to('cuda')\n","# create a new variable using the same tensor data\n","x = Variable(input_var.data, requires_grad=True).to('cuda')\n","# get the model output and save it to prevent any modifications\n","print(torch.cuda.memory_allocated('cuda'))\n","out = original(x)\n","print(torch.cuda.memory_allocated('cuda'))\n","out_not_checkpointed = out.data.clone()\n","# calculate the gradient now and save the parameter gradients values\n","original.zero_grad()\n","out.sum().backward()\n","grad_not_checkpointed = {}\n","for name, param in original.named_parameters():\n","    print(name,param)\n","    grad_not_checkpointed[name] = param.grad.data.clone()\n","grad_not_checkpointed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VYSjmta1cIp1"},"source":["print(output_checkpointed)\n","print(out_not_checkpointed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C5HxtcuMaICc"},"source":["grad_checkpointed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cVpMUilWf2KP"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable, Function\n","import torch.utils.checkpoint as checkpoint\n","from collections import OrderedDict\n","class ConvBNReLU(nn.Module):\n","    \n","    def __init__(self, in_planes, out_planes):\n","        \n","        super(ConvBNReLU, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_planes)\n","        self.relu1 = nn.ReLU(inplace=True)\n","    \n","    def forward(self, x):\n","        out = self.relu1(self.bn1(self.conv1(x)))\n","        return out\n","\n","class DummyNet(nn.Module):\n","    def __init__(self):\n","        super(DummyNet, self).__init__()\n","        self.features = nn.Sequential(OrderedDict([\n","            ('conv1', nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)),\n","            ('bn1', nn.BatchNorm2d(16)),\n","            ('relu1', nn.ReLU(inplace=True)),\n","        ]))\n","\n","        # The module that we want to checkpoint\n","        self.module = ConvBNReLU(16, 64) \n","        \n","        self.final_module = ConvBNReLU(64, 64)\n","    \n","    def custom(self, module):\n","        def custom_forward(*inputs):\n","            inputs = module(inputs[0])\n","            return inputs\n","        return custom_forward\n","    \n","    def forward(self, x):\n","        print(torch.cuda.memory_allocated('cuda'))\n","        out1 = self.features(x)\n","        print(torch.cuda.memory_allocated('cuda'))\n","#        out2 = self.module(out1)\n","        out2 = checkpoint.checkpoint(self.custom(self.module), out1)\n","        print(torch.cuda.memory_allocated('cuda'))\n","        out3 = self.final_module(out2)\n","        print(torch.cuda.memory_allocated('cuda'))\n","        return out3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUw1pFIWgBXC"},"source":["torch.cuda.reset_max_memory_allocated('cuda')\n","print(torch.cuda.memory_allocated('cuda'))\n","dn=DummyNet().to('cuda')\n","print(torch.cuda.memory_allocated('cuda'))\n","x = Variable(torch.randn(1,3,10,10), requires_grad=True).to('cuda')\n","print(torch.cuda.memory_allocated('cuda'))\n","outi=dn(x)\n","print(torch.cuda.memory_allocated('cuda'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Px426vSXgytE"},"source":["488960\n","519168\n","520704\n","520704\n","535040\n","587264\n","639488\n","516608"],"execution_count":null,"outputs":[]}]}